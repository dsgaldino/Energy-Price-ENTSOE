{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e15106",
   "metadata": {},
   "source": [
    "# Energy Price\n",
    "\n",
    "The code is for an application that retrieves price information from the Entsoe transparency portal. It fetches price data starting from a requested date up to the current date. The values are converted from Megawatt-hour (MWh) to Kilowatt-hour (kWh).\n",
    "\n",
    "The code imports the necessary libraries, defines the URL for accessing the price data, and defines two functions: **update_dataset** for updating the dataset and **convert_date** for converting values to date format.\n",
    "\n",
    "Next, the code reads a CSV file named 'EntsoeEnergyPrice.csv' into a DataFrame called **df** and converts the **'Date'** column to date format.\n",
    "\n",
    "Then, it determines the oldest and most recent dates present in the dataset, as well as today's date. The code checks if the dataset needs to be updated based on the difference between the most recent date and today.\n",
    "\n",
    "After that, it creates lists of dates: **datesL** for the most recent dates and **datesE** for the oldest dates. The code retrieves the price information by calling the update_dataset function with the appropriate dates and updates the **df_comb** DataFrame.\n",
    "\n",
    "Following that, the code performs data cleaning steps such as renaming columns, splitting columns, and converting price units.\n",
    "\n",
    "Finally, the code concatenates the original DataFrame (**df**) with the updated DataFrame (**df_comb**), corrects the date formatting, and saves the resulting DataFrame to a CSV file named **'EntsoeEnergyPrice.csv'**.\n",
    "\n",
    "*It's important to note that the code assumes the existence of the 'EntsoeEnergyPrice.csv' CSV file and may require modifications or additional dependencies to run correctly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7869154",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e7a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fbd99",
   "metadata": {},
   "source": [
    "#### Define the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88eb2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "start_url = 'https://transparency.entsoe.eu/transmission-domain/r2/dayAheadPrices/show?name=&defaultValue=false&viewType=TABLE&areaType=BZN&atch=false&dateTime.dateTime='\n",
    "end_url = '+00:00|CET|DAY&biddingZone.values=CTY|10YNL----------L!BZN|10YNL----------L&resolution.values=PT60M&dateTime.timezone=CET_CEST&dateTime.timezone_input=CET+(UTC+1)+/+CEST+(UTC+2)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662b5c5",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cf2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(dates, url_format, df_comb):\n",
    "    for actual_date in dates:\n",
    "        # Create a URL with the new date\n",
    "        url = start_url + actual_date.replace(\"-\", \".\") + end_url\n",
    "\n",
    "        # Copy the information from the URL into a temporary DataFrame\n",
    "        df_temp = pd.read_html(url)[0]\n",
    "\n",
    "        # Add a Date column to the temporary DataFrame\n",
    "        df_temp['Date'] = pd.to_datetime(actual_date, format=url_format).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Append the information from the temporary DataFrame to the combined DataFrame\n",
    "        df_comb = pd.concat([df_comb, df_temp], ignore_index=True)\n",
    "\n",
    "    return df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317277cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(value):\n",
    "    try:\n",
    "        return pd.to_datetime(value).date()\n",
    "    except ValueError:\n",
    "        return np.nan   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e16d7a",
   "metadata": {},
   "source": [
    "#### Read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa0a17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Import Grid (EUR/kWh)</th>\n",
       "      <th>Export Grid (EUR/kWh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>23:00</td>\n",
       "      <td>0.09094</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>22:00</td>\n",
       "      <td>0.10047</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>21:00</td>\n",
       "      <td>0.10713</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>20:00</td>\n",
       "      <td>0.12171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>19:00</td>\n",
       "      <td>0.10711</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date   Hour  Import Grid (EUR/kWh)  Export Grid (EUR/kWh)\n",
       "0 2023-05-25  23:00                0.09094                    NaN\n",
       "1 2023-05-25  22:00                0.10047                    NaN\n",
       "2 2023-05-25  21:00                0.10713                    NaN\n",
       "3 2023-05-25  20:00                0.12171                    NaN\n",
       "4 2023-05-25  19:00                0.10711                    NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the original dataset\n",
    "df = pd.read_csv('EntsoeEnergyPrice.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ec6f4",
   "metadata": {},
   "source": [
    "#### Define the Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca579957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date found in the dataset: 2019-11-19\n",
      "Latest date found in the dataset: 2023-06-01\n",
      "Today's date: 2023-06-02\n"
     ]
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# The earliest date in the dataset\n",
    "min_date = df['Date'].min().date()\n",
    "\n",
    "# The latest date in the dataset\n",
    "max_date = df['Date'].max().date()\n",
    "\n",
    "# Today's date\n",
    "today = datetime.today().date()\n",
    "\n",
    "print('Earliest date found in the dataset:', min_date)\n",
    "print('Latest date found in the dataset:', max_date)\n",
    "print('Today\\'s date:', today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ffb34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between the latest date and today is: 1 days\n",
      "The list of latest days to be collected.\n",
      "['02-06-2023']\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Updated the Earliest Dates\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_date \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2012-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdate():\n\u001b[1;32m---> 31\u001b[0m     df_comb \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatesE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_comb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Print some information about the Earliest dates    \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe list of Earliest days to be collected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mupdate_dataset\u001b[1;34m(dates, url_format, df_comb)\u001b[0m\n\u001b[0;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m start_url \u001b[38;5;241m+\u001b[39m actual_date\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m end_url\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Copy the information from the URL into a temporary DataFrame\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df_temp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Add a Date column to the temporary DataFrame\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(actual_date, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39murl_format)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:1212\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend)\u001b[0m\n\u001b[0;32m   1208\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m   1210\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:981\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 981\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:257\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:798\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio):\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 798\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;66;03m# try to parse the input in the simplest way\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     r \u001b[38;5;241m=\u001b[39m parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio, parser\u001b[38;5;241m=\u001b[39mparser)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lxml\\html\\__init__.py:937\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(filename_or_url, parser, base_url, **kw)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     parser \u001b[38;5;241m=\u001b[39m html_parser\n\u001b[1;32m--> 937\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m etree\u001b[38;5;241m.\u001b[39mparse(filename_or_url, parser, base_url\u001b[38;5;241m=\u001b[39mbase_url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32msrc/lxml/etree.pyx:3538\u001b[0m, in \u001b[0;36mlxml.etree.parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:1897\u001b[0m, in \u001b[0;36mlxml.etree._parseDocument\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:1917\u001b[0m, in \u001b[0;36mlxml.etree._parseFilelikeDocument\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:1811\u001b[0m, in \u001b[0;36mlxml.etree._parseDocFromFilelike\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:1201\u001b[0m, in \u001b[0;36mlxml.etree._BaseParser._parseDocFromFilelike\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:615\u001b[0m, in \u001b[0;36mlxml.etree._ParserContext._handleParseResultDoc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:721\u001b[0m, in \u001b[0;36mlxml.etree._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/etree.pyx:333\u001b[0m, in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:370\u001b[0m, in \u001b[0;36mlxml.etree._FileReaderContext.copyToBuffer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:587\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_left:\n\u001b[1;32m--> 587\u001b[0m     value\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m chunk_left \u001b[38;5;241m-\u001b[39m amt\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:630\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \n\u001b[0;32m    626\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create list of the days that are missing\n",
    "datesL = []  # Latest Dates\n",
    "datesE = []  # Earliest Dates\n",
    "\n",
    "# Create a temporary DataFrame to store the daily price \n",
    "df_comb = pd.DataFrame() \n",
    "\n",
    "# Days of difference between today and latest date\n",
    "dif_days = (today - max_date).days\n",
    "\n",
    "# Check if the dataset is updated\n",
    "if dif_days != 0:\n",
    "    \n",
    "    # Create Latest Dates\n",
    "    datesL = [(max_date + timedelta(days=i+1)).strftime(\"%d-%m-%Y\") for i in range(dif_days)]\n",
    "    \n",
    "    # Updated the Latest Dates\n",
    "    df_comb = update_dataset(datesL, \"%d-%m-%Y\", df_comb)\n",
    "    \n",
    "    # Print some information about the latest dates\n",
    "    print('The difference between the latest date and today is:', dif_days, 'days')\n",
    "    print(\"The list of latest days to be collected.\")\n",
    "    print(datesL)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Create Earliest Dates\n",
    "    datesE = [(min_date - timedelta(days=i)).strftime(\"%d-%m-%Y\") for i in range(1, 11)]\n",
    "    \n",
    "    # Updated the Earliest Dates\n",
    "    if min_date >= pd.to_datetime(\"2012-01-01\").date():\n",
    "        df_comb = update_dataset(datesE, \"%d-%m-%Y\", df_comb)\n",
    "    \n",
    "    # Print some information about the Earliest dates    \n",
    "    print(\"The list of Earliest days to be collected.\")\n",
    "    print(datesE)\n",
    "    print('\\n')\n",
    "    \n",
    "else:\n",
    "    # Print some information about the latest dates\n",
    "    print('The dataset is updated with the most recent data!')\n",
    "    print('\\n')\n",
    "    \n",
    "    # Create Earliest Dates\n",
    "    datesE = [(min_date - timedelta(days=i)).strftime(\"%d-%m-%Y\") for i in range(1, 71)] # Change the days range\n",
    "    \n",
    "    # Updated the Earliest Dates\n",
    "    if min_date >= pd.to_datetime(\"2012-01-01\").date():\n",
    "        df_comb = update_dataset(datesE, \"%d-%m-%Y\", df_comb)    \n",
    "    \n",
    "    # Print some information about the Earliest dates\n",
    "    print(\"The list of Earliest days to be collected.\")\n",
    "    print(datesE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715f7bc",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2062a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first level of columns\n",
    "df_comb.columns = df_comb.columns.droplevel()\n",
    "\n",
    "# Rename the columns\n",
    "column_mapping = {\n",
    "    \"index\": \"Index\",\n",
    "    \"MTU\": \"start-end [time]\",\n",
    "    \"Day-ahead Price\": \"Import Grid (EUR/kWh)\",\n",
    "    \"\": \"Date\"\n",
    "}\n",
    "\n",
    "df_comb.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Split the hours in start and end\n",
    "df_comb[['Hour', 'End']] = df_comb['start-end [time]'].str.split('-', expand=True)\n",
    "\n",
    "# Remove blank spaces\n",
    "df_comb['Hour'] = df_comb['Hour'].str.strip()\n",
    "df_comb['End'] = df_comb['End'].str.strip()\n",
    "\n",
    "# Remove the original column\n",
    "df_comb.drop('start-end [time]', axis=1, inplace=True)\n",
    "df_comb.drop('End', axis=1, inplace=True)\n",
    "\n",
    "# Converting MWh to kWh and Drop the old colunm\n",
    "df_comb['Import Grid (EUR/kWh)'] = df_comb['[EUR / MWh]'] / 1000\n",
    "df_comb.drop('[EUR / MWh]', axis=1, inplace=True)\n",
    "\n",
    "# Creating a new column \"Export Grid (EUR/kWh)\" that is a copy of 'Import Grid (EUR/kWh)'\n",
    "df_comb['Export Grid (EUR/kWh)'] = df_comb['Import Grid (EUR/kWh)']\n",
    "\n",
    "# Replace values greater than zero with NaN in the \"Export Grid (EUR/kWh)\" column\n",
    "df_comb.loc[df_comb['Import Grid (EUR/kWh)'] < 0, 'Import Grid (EUR/kWh)'] = np.nan\n",
    "\n",
    "# Replace values less than or equal to zero with NaN in the \"Import Grid (EUR/kWh)\" column\n",
    "df_comb.loc[df_comb['Export Grid (EUR/kWh)'] > 0, 'Export Grid (EUR/kWh)'] = np.nan\n",
    "df_comb.loc[df_comb['Export Grid (EUR/kWh)'] < 0, 'Export Grid (EUR/kWh)'] = df_comb['Export Grid (EUR/kWh)'] * (-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e37204",
   "metadata": {},
   "source": [
    "#### Concatenate the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5643fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the information from the temporary DataFrame to the combined DataFrame\n",
    "df = pd.concat([df, df_comb], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b2744",
   "metadata": {},
   "source": [
    "#### Correction of the Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22bac378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51066424",
   "metadata": {},
   "source": [
    "#### Salving the date in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f67f2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a CSV file\n",
    "df.to_csv('EntsoeEnergyPrice.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a01bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the success message\n",
    "print('DATA UPDATED SUCCESSFULLY!')\n",
    "\n",
    "# Pause for 10 seconds\n",
    "time.sleep(10)\n",
    "\n",
    "# Close the program\n",
    "sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
